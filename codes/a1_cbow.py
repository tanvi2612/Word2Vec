# -*- coding: utf-8 -*-
"""a1_cbow

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hW5M606FyJkFkoMWTPGDG61Zm7pKH3DK
"""

import numpy as np
import pandas as pd
import json
import re
# import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import defaultdict

vocab_size = 0
embedding_size = 10

# Opening JSON file 
f = open('/content/drive/My Drive/reviews_Electronics_5.json',) 

records = [json.dumps(line) for line in f]
final_records = [json.loads(lines) for lines in records]
records = len(final_records) 
print(records)

# Closing file 
f.close()

small_data = []
for i in range(records):
  small_data.append(json.loads(final_records[i]))



stopWords = stopwords.words('english') 
def preprocessing():
  data = []
  TOTAL = 2
  f = open('vocab.txt', 'w')
  vocab['BEGIN'] = 0
  vocab['END'] = 1
  f.write(str('BEGIN') + ' '  + str(0) + '\n')
  f.write(str('END') + ' '  + str(1) + '\n')


  for w in range(records):
    temp = small_data[w]['reviewText']
    temp = re.sub(r'[!\(\)\[\]\{\};:\'"\,\<\>/?@#$%^&*_~+`]', ' ', temp)    
    # sentence = re.sub(r'[.]', ' ', data[w])
    sentences = temp.split('.')
    for sentence in sentences:
      add_data = []  
      sentence = sentence.split()
      for s in sentence:
        s = s.lower()
        s = s.strip("-")
        # # print(s)AL
        #     # TOTAL += 1/content/drive/My Drive/
        # add_data.append(s)
        # data.append([add_data])

        if s not in stopWords:
          if s not in vocab.keys():
            vocab[s] = TOTAL
            f.write(str(s) + ' '  + str(TOTAL) + '\n')
            TOTAL += 1
          add_data.append(s)
      data.append([add_data])
      # print(add_data)

    # print(data[w])
  # print(TOTAL)
  global vocab_size
  vocab_size = TOTAL
  # print(vocab_size)
  f.close()
  return data

def softmax(x):
  sum1 = np.sum(np.exp(x))
  return [np.exp(i)/sum1 for i in x] 

def training_CBOW(input, output_word, context, embedding, learning_rate=0.001):
  inp = np.zeros((vocab_size,1)) # V X 1
  inp_size = len(input)
  for word in input:
    inp[vocab[word]] = 1/inp_size
  h = np.dot(context.T, inp) # V X N , V X 1 = N X 1 
  z = np.dot(embedding, h) # V X N , N X 1 = V X 1
  a = softmax(z) #V X 1
  a[vocab[output_word]] -= 1
  dembedding = np.dot(a, h.T) # V X N
  dcontext = np.dot(inp, (np.dot(embedding.T, a)).T)   # V X N
  embedding -= learning_rate * dembedding
  context -= learning_rate * dcontext

def dumpinfile():
    
  with open('context_cbow.txt', 'w') as f:
    for x in range(vocab_size):
      f.write(str(context[x]) + '\n') 
    f.write('\n\n')
  with open('embedding_cbow.txt', 'w') as f:
    for x in range(vocab_size):
      f.write(str(embedding[x]) + '\n') 
    f.write('\n\n')

def train_CBOW(dataset, context, embedding, window = 2):
  context_size = window * 2
  itr = 0
  for sentence in dataset :    
    itr += 1
    if itr % 5000 == 0: 
      print('Iteration:', itr)     
      dumpinfile()
    for s in sentence:
      if not s:
        continue
      max_len = len(s)
      if max_len >= 3:
        s1 = ['BEGIN', 'BEGIN', s[1], s[2]]
        training_CBOW(s1, s[0], context, embedding)   
        e1 = [s[max_len - 3], s[max_len - 2], 'END', 'END']
        training_CBOW(e1, s[max_len-1], context, embedding)         
      else:
        if max_len == 2:
          s1 = ['BEGIN', 'BEGIN', s[1], 'END']  
          training_CBOW(s1, s[0], context, embedding)           
          e1 = ['BEGIN', s[0], 'END', 'END']
          training_CBOW(e1, s[max_len-1], context, embedding)          
          continue
        elif max_len == 1:
          s1 = ['BEGIN', 'BEGIN', 'END', 'END']
          training_CBOW(s1, s[0], context, embedding)   
          continue
      if max_len >= 4:
        s2 = ['BEGIN', s[0], s[2], s[3]]
        training_CBOW(s2, s[1], context, embedding)   
        e2 = [s[max_len - 4], s[max_len - 3], s[max_len - 1], 'END']
        training_CBOW(e2, s[max_len-2], context, embedding)   
              
      for ind in range(2,max_len-2):
        add_cxt = []
        start = max(0, ind-window)
        end = min(max_len-1, ind+window)
        for i in range(start,end + 1):
          if i == ind:
            continue
          else:
            add_cxt.append(s[i])
        training_CBOW(add_cxt, s[ind], context, embedding)   
        
  # print(contet)

vocab = {}

dataset = preprocessing()
# context = np.random.rand(vocab_size, embedding_size)
# embedding = np.random.rand(vocab_size, embedding_size)

# train_CBOW(dataset, context, embedding)
# print(context['premium'])

# print(final_records[6])
# print(small_data[6])
# print(dataset[6])