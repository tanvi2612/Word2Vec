# -*- coding: utf-8 -*-
"""a1_sgns.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12PyiRG6_eov9mTWhjDIrkXXUSr926cl6
"""

import numpy as np
import pandas as pd
import json
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords
from collections import defaultdict

vocab_size = 0
embedding_size = 10

# Opening JSON file 
f = open('/content/drive/My Drive/reviews_Electronics_5.json',) 

records = [json.dumps(line) for line in f]
final_records = [json.loads(lines) for lines in records]
print(len(final_records))

# Closing file 
f.close()

small_data = []
for i in range(100):
  small_data.append(json.loads(final_records[i]))

stopWords = stopwords.words('english') 
def preprocessing():
  data = []
  TOTAL = 2
  vocab['BEGIN'] = 0
  vocab['END'] = 1
  for w in range(100):
    temp = small_data[w]['reviewText']
    temp = re.sub(r'[!\(\)\[\]\{\};:\'"\,\<\>/?@#$%^&*_~+`]', ' ', temp)    
    # sentence = re.sub(r'[.]', ' ', data[w])
    sentences = temp.split('.')
    for sentence in sentences:
      add_data = []  
      sentence = sentence.split()
      for s in sentence:
        s = s.lower()
        s = s.strip("-")
        # # print(s)AL
        #     # TOTAL += 1
        # add_data.append(s)
        # data.append([add_data])

        if s not in stopWords:
          if s not in vocab.keys():
            vocab[s] = TOTAL
            TOTAL += 1
          add_data.append(s)
      data.append([add_data])
      # print(add_data)

    # print(data[w])
  # print(TOTAL)
  global vocab_size
  vocab_size = TOTAL
  # print(vocab_size)
  
  return data

def softmax(x):
  sum1 = np.sum(np.exp(x))
  return [np.exp(i)/sum1 for i in x] 

def sigmoid(value):
    if -value > np.log(np.finfo(type(value)).max):
        return 0.0    
    return 1.0/ (1.0 + np.exp(-value))

def training_CBOW(input, output_word, context, embedding, learning_rate=0.001):
  inp = np.zeros((vocab_size,1)) # V X 1
  inp_size = len(input)
  for word in input:
    inp[vocab[word]] = 1/inp_size
  h = np.dot(context.T, inp) # V X N , V X 1 = N X 1 
  z = np.dot(embedding, h) # V X N , N X 1 = V X 1
  a = softmax(z) #V X 1
  a[vocab[output_word]] -= 1
  dembedding = np.dot(a, h.T) # V X N
  dcontext = np.dot(inp, (np.dot(embedding.T, a)).T)   # V X N
  embedding -= learning_rate * dembedding
  context -= learning_rate * dcontext 
  # return context, embedding

def training_SGNS(output_words, input, context, embedding, learning_rate=0.001, negative_sample=4):
  targets = []
  inp = embedding[vocab[input]]
  for word in output_words:
    targets.append(vocab[word])
    target = context[vocab[word]]
    h = np.dot(inp, target.T)
    z = sigmoid(h)
    out = (1 - z) * learning_rate
    embedding[vocab[input]] += target * out
    context[vocab[word]] += inp * out
  neg = np.random.randint(0, vocab_size-1, negative_sample)
  for word in neg:
    if word in targets:
      word = max(min(targets)+4, vocab_size -1)
      if word in targets: 
        word = min(max(targets) - 4, 0)
        if word in targets:
          continue 
    
    target = context[word]
    h = np.dot(inp, target.T)
    z = sigmoid(h)
    out = (- z) * learning_rate
    embedding[vocab[input]] += target * out
    context[word] += inp * out

def train_SGNS(dataset, context, embedding, window = 2):
  context_size = window * 2
  itr = 0
  for sentence in dataset :
    itr += 1
    if itr % 500 == 0:      
      dumpinfile()
    for s in sentence:
      if not s:
        continue
      max_len = len(s)
      if max_len >= 3:
        s1 = ['BEGIN', 'BEGIN', s[1], s[2]]
        training_SGNS(s1, s[0], context, embedding)   
        e1 = [s[max_len - 3], s[max_len - 2], 'END', 'END']
        training_SGNS(e1, s[max_len-1], context, embedding)         
      else:
        if max_len == 2:
          s1 = ['BEGIN', 'BEGIN', s[1], 'END']  
          training_SGNS(s1, s[0], context, embedding)           
          e1 = ['BEGIN', s[0], 'END', 'END']
          training_SGNS(e1, s[max_len-1], context, embedding)          
          continue
        elif max_len == 1:
          s1 = ['BEGIN', 'BEGIN', 'END', 'END']
          training_SGNS(s1, s[0], context, embedding)   
          continue
      if max_len >= 4:
        s2 = ['BEGIN', s[0], s[2], s[3]]
        training_SGNS(s2, s[1], context, embedding)   
        e2 = [s[max_len - 4], s[max_len - 3], s[max_len - 1], 'END']
        training_SGNS(e2, s[max_len-2], context, embedding)   
              
      for ind in range(2,max_len-2):
        add_cxt = []
        start = max(0, ind-window)
        end = min(max_len-1, ind+window)
        for i in range(start,end + 1):
          if i == ind:
            continue
          else:
            add_cxt.append(s[i])
        training_SGNS(add_cxt, s[ind], context, embedding)   
        
  # print(contet)

# np.random.seed(10)
vocab = {}
dataset = preprocessing()
context = np.random.rand(vocab_size, embedding_size)
embedding = np.random.rand(vocab_size, embedding_size)
  # return context, embedding

    
train_SGNS(dataset, context, embedding)

def dumpinfile():
  with open('output.txt', 'w') as f:
    for x in range(vocab_size):
      f.write(str(context[x]) + '\n') 
    f.write('\n\n')
    for x in range(vocab_size):
      f.write(str(embedding[x]) + '\n') 
    f.write('\n\n')

print(vocab_size)
print(context[8262])